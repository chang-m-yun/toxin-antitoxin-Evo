{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING"
      ],
      "metadata": {
        "id": "Yu0b5QleJfKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U trl transformers accelerate peft\n",
        "!pip install -q datasets bitsandbytes einops\n",
        "!pip install -q flash_attn\n",
        "!pip install accelerate\n",
        "#!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
        "import gc"
      ],
      "metadata": {
        "id": "VVRXaaQXZPw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HF_Evo():\n",
        "\n",
        "    model_name: str = \"togethercomputer/evo-1-8k-base\"\n",
        "    device: str = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    revision: str = \"1.1_fix\"\n",
        "\n",
        "    def __init__(self, model_name=None, revision=None):\n",
        "\n",
        "        if model_name is not None:\n",
        "          self.model_name = model_name\n",
        "        else:\n",
        "          print('Model name needed! Using default: ' + self.model_name)\n",
        "        if revision is not None:\n",
        "          self.revision = revision\n",
        "\n",
        "        self.config = AutoConfig.from_pretrained(self.model_name,\n",
        "                                            trust_remote_code=True,\n",
        "                                            revision=self.revision)\n",
        "\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            config=self.config,\n",
        "            trust_remote_code=True,\n",
        "            #load_in_8bit=False,\n",
        "            #torch_dtype=torch.float16,\n",
        "            revision=self.revision).to(self.device)\n",
        "\n",
        "\n",
        "        self.model.config.use_cache = True\n",
        "        self.model.eval()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name,\n",
        "                                                      trust_remote_code=True)\n",
        "\n",
        "        self.tokenizer.add_special_tokens({'eos_token': ' '})\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        print('Tokenizer pad token:', self.tokenizer.pad_token)\n",
        "        print('Tokenizer eos token:', self.tokenizer.eos_token)\n",
        "\n",
        "def run_model(model, tokenizer, prompt, max_new_tokens=1000, temp=1, rep_penalty=None,\n",
        "              top_k=4, top_p=1, alpha=None, device='cuda:0'):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    del input_ids['token_type_ids']\n",
        "    outputs = model.generate(\n",
        "            **input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temp,\n",
        "            repetition_penalty=rep_penalty,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            penalty_alpha=alpha,\n",
        "            do_sample=temp is not None,\n",
        "            eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "rUvm0vWZZPy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  evo = None\n",
        "  evo = HF_Evo()\n",
        "except:\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  if evo is not None:\n",
        "    del evo.model, evo.tokenizer, evo\n",
        "  evo = HF_Evo()"
      ],
      "metadata": {
        "id": "w4YrMWpNk1JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evo.model.dtype"
      ],
      "metadata": {
        "id": "yxMES7l_3l-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test the model is working properly with known prompts but also not working when prompted with TA special tokens\n",
        "prompt = \"@!\"\n",
        "run_model(evo.model, evo.tokenizer,prompt)"
      ],
      "metadata": {
        "id": "kBX5MVh4k4XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to read files that have everything in one line and error in json.loads()\n",
        "import json\n",
        "\n",
        "f = open('/content/training_data_5k_2.json')\n",
        "doc = f.read()\n",
        "x = doc.split('\"')\n",
        "seqs = x[7::8]\n",
        "f.close()\n",
        "\n",
        "dataset = []\n",
        "for seq in seqs:\n",
        "  #temp = seq.replace('`','>')\n",
        "  dataset.append({'text':seq})"
      ],
      "metadata": {
        "id": "eLEN0I1-L_bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to read files already as json\n",
        "\n",
        "#import json\n",
        "\n",
        "#f = open('/content/training_data_5k.json')\n",
        "#doc = f.read()\n",
        "#dataset = json.loads(doc)\n",
        "#f.close()\n",
        "#print(len(dataset), dataset[0])"
      ],
      "metadata": {
        "id": "LkL73i6qJ4Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #to read files with individual dicts of sequence per line\n",
        "\n",
        "# import json\n",
        "\n",
        "# f = open('/content/training_data.json')\n",
        "# dataset = []\n",
        "# lines = f.readlines()\n",
        "# for l in lines:\n",
        "#  json_obj = json.loads(l)\n",
        "#  dataset.append(json_obj)\n",
        "# f.close()\n",
        "# dataset[0]"
      ],
      "metadata": {
        "id": "6vezsW0j6uzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "c=0\n",
        "MAX_LENGTH = 1024 #4096 for A100\n",
        "filtered_dataset = []\n",
        "dataset_size = 10000\n",
        "\n",
        "for i,d in enumerate(dataset):\n",
        "\n",
        "  if i>=dataset_size:\n",
        "    break\n",
        "\n",
        "  t = d['text'].strip()\n",
        "  if len(t) > MAX_LENGTH:\n",
        "    c=c+1\n",
        "    continue\n",
        "  temp = evo.tokenizer(t, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=MAX_LENGTH)\n",
        "  temp['input_ids'] = copy.deepcopy(temp['input_ids'][0])\n",
        "  temp['attention_mask'] = copy.deepcopy(temp['attention_mask'][0])\n",
        "  temp['token_type_ids'] = copy.deepcopy(temp['token_type_ids'][0])\n",
        "  temp[\"labels\"] = copy.deepcopy(temp['input_ids'])\n",
        "  temp['text'] = t\n",
        "  #temp['record'] = copy.deepcopy(d['record'])\n",
        "\n",
        "  filtered_dataset.append(temp)\n",
        "\n",
        "print('Seqs longer than max_length:',c)\n",
        "print(len(filtered_dataset), filtered_dataset[0]['text'])"
      ],
      "metadata": {
        "id": "zG5QwZNpjkYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "dataset = Dataset.from_list(filtered_dataset)"
      ],
      "metadata": {
        "id": "5RY1RW--73Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_size = int(len(dataset)*0.2)\n",
        "split_dataset = dataset.train_test_split(test_size=test_size, seed=0)"
      ],
      "metadata": {
        "id": "twXT4_bLDtTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset"
      ],
      "metadata": {
        "id": "p8nMyISv5qAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DefaultDataCollator\n",
        "#Data collator\n",
        "data_collator = DefaultDataCollator(return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "z4Xyacpl8RLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_layers=[]\n",
        "for n,m in evo.model.named_modules():\n",
        "  if \"Linear\" in str(type(m)):\n",
        "    linear_layers.append(n)\n",
        "print(linear_layers, len(linear_layers))"
      ],
      "metadata": {
        "id": "YLNxZGvD9lSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#select only certain layers (MLPs layers or MHA layers or both, etc)\n",
        "mlp_layers=[]\n",
        "for ll in linear_layers:\n",
        "  #if \"mlp\" in ll or \"mha\" in ll:\n",
        "  if \"mha\" in ll:\n",
        "    mlp_layers.append(ll)\n",
        "print(len(mlp_layers[:]), mlp_layers[:])\n"
      ],
      "metadata": {
        "id": "Ru0lOfQyseoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_alpha = 64 # thumb rule is 2x of r https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms\n",
        "lora_dropout = 0.1 # 0.05 recomended\n",
        "lora_r = 64 # between 8 and 16 because of resrouces available (change maybe?)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "                 r = lora_r, # the dimension of the low-rank matrices\n",
        "                 lora_alpha = lora_alpha, # scaling factor for the weight matrices\n",
        "                 lora_dropout = lora_dropout, # dropout probability of the LoRA layers\n",
        "                 bias=\"none\", #we can change this to change performance\n",
        "                 #task_type=\"CAUSAL_LM\",          #could also not include this\n",
        "                 target_modules=mlp_layers,\n",
        "                 init_lora_weights = 'gaussian',\n",
        "                 #is_prompt_learning=True\n",
        "                 )\n",
        "\n",
        "## more data her: https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraConfig"
      ],
      "metadata": {
        "id": "ambL_KREw5Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import bitsandbytes\n",
        "\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 3e-4\n",
        "MODEL_SAVE_FOLDER_NAME = \"lora_evo_ta_all_layers_18_attention_layers\"\n",
        "training_args = TrainingArguments(\n",
        "                    output_dir=MODEL_SAVE_FOLDER_NAME,\n",
        "                    overwrite_output_dir=True,\n",
        "                    warmup_steps=500,\n",
        "                    gradient_accumulation_steps=1,\n",
        "                    per_device_train_batch_size=1,\n",
        "                    per_device_eval_batch_size=1,\n",
        "                    learning_rate=LEARNING_RATE,\n",
        "                    num_train_epochs=EPOCHS,\n",
        "                    logging_strategy=\"steps\",\n",
        "                    evaluation_strategy=\"steps\",\n",
        "                    eval_steps=1200,\n",
        "                    logging_steps=1200,\n",
        "                    save_strategy=\"epoch\",\n",
        "                    log_level = 'debug',\n",
        "                    logging_dir = './log/',\n",
        "                    do_train = True,\n",
        "                    do_eval = True,\n",
        "                    lr_scheduler_type = \"constant\",\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "ZWke3RK-67ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evo_peft_model = get_peft_model(evo.model, lora_config)\n",
        "evo_peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "eGiQId8oEZis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evo.tokenizer.pad_token = evo.tokenizer.eos_token\n",
        "trainer = Trainer(\n",
        "        model=evo_peft_model,\n",
        "        tokenizer=evo.tokenizer,\n",
        "        args=training_args,\n",
        "        train_dataset=split_dataset['train'],\n",
        "        eval_dataset=split_dataset['test'],\n",
        "        data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.can_return_loss = True\n",
        "\n",
        "evo_peft_model.config.use_cache = False\n",
        "\n",
        "#for name, module in trainer.model.named_modules():\n",
        "#    if \"norm\" in name:\n",
        "#        module = module.to(torch.float32)"
      ],
      "metadata": {
        "id": "mvsLXOZKMO-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "kpWiAj_0_yWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "## only saves the incremental ðŸ¤— PEFT weights (adapter_model.bin) that were trained, meaning it is super efficient to store, transfer, and load.\n",
        "#trainer.model.save_pretrained(MODEL_SAVE_FOLDER_NAME)\n",
        "## save the full model and the training arguments\n",
        "#trainer.save_model(MODEL_SAVE_FOLDER_NAME)\n",
        "#trainer.model.config.save_pretrained(MODEL_SAVE_FOLDER_NAME)\n",
        "evo_peft_model.config.use_cache = True"
      ],
      "metadata": {
        "id": "DhjvmCVdOGfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_params = {n: p for n, p in trainer.model.named_parameters() if \"lora_B\" in n}\n",
        "for n, p in lora_params.items():\n",
        "    print(n, p.sum())"
      ],
      "metadata": {
        "id": "Y8t54poyLD10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset['test'][100]['text']"
      ],
      "metadata": {
        "id": "IkuxaqVTE5Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"!!\"\n",
        "run_model(trainer.model, trainer.tokenizer, prompt)"
      ],
      "metadata": {
        "id": "K6NE0oOiyf4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"``\"\n",
        "run_model(trainer.model, trainer.tokenizer, prompt)"
      ],
      "metadata": {
        "id": "SQww3ke9SGvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"@@\"\n",
        "run_model(trainer.model, trainer.tokenizer, prompt)"
      ],
      "metadata": {
        "id": "9BV0wPqZU7x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"!ATG\"\n",
        "run_model(trainer.model, trainer.tokenizer, prompt)"
      ],
      "metadata": {
        "id": "7vT85dPBU-kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "HF_token = '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'\n",
        "\n",
        "#evo_peft_model.save_pretrained(\"trained_\"+MODEL_SAVE_FOLDER_NAME)\n",
        "#trainer.model.save_pretrained(\"trained_from_trainer_\"+MODEL_SAVE_FOLDER_NAME)\n",
        "#trainer.tokenizer.save_pretrained(\"tokenizer_\"+MODEL_SAVE_FOLDER_NAME)\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "9JcPGagEuVch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(\"lsmille/\"+MODEL_SAVE_FOLDER_NAME)"
      ],
      "metadata": {
        "id": "nhFcj1af3mJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset['train'].to_json(MODEL_SAVE_FOLDER_NAME+\"_train.jsonl\")\n",
        "split_dataset['test'].to_json(MODEL_SAVE_FOLDER_NAME+\"_test.jsonl\")"
      ],
      "metadata": {
        "id": "1K8VF_1H9HFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q9PPCZpgDcFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kdyfa0uUcrVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b6KDibK5crTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hwulf3h4crQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jNCGC4hrcrON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Validation***"
      ],
      "metadata": {
        "id": "k3AbDUTlDc4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restart session and download finetuned model"
      ],
      "metadata": {
        "id": "b7Ls9XKGDSSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U trl transformers accelerate peft\n",
        "!pip install -q datasets bitsandbytes einops\n",
        "!pip install -q flash_attn\n",
        "!pip install accelerate\n",
        "#!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
        "import gc"
      ],
      "metadata": {
        "id": "PrKvo7qDDyd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evo = HF_Evo()"
      ],
      "metadata": {
        "id": "eFh0oF5FeC3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evo.model.eval()"
      ],
      "metadata": {
        "id": "BltFCGQgeFsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_model(evo.model, evo.tokenizer, \"``\")"
      ],
      "metadata": {
        "id": "8j2ZuQTEeXVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evo.model.config.use_cache = False\n",
        "\n",
        "adapter = 'lsmille/lora_evo_ta_all_layers_13'\n",
        "evo.model.load_adapter(adapter)\n",
        "evo.model.config.use_cache = True\n",
        "evo.model.eval()"
      ],
      "metadata": {
        "id": "cMhVy2XoeMRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_model(evo.model, evo.tokenizer, \"!!\")"
      ],
      "metadata": {
        "id": "MNEYHI5jeeU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AR-gDU4geiKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TnSeV12CeiFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qCF6SWdheiCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sxoaFHSEeiAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'lsmille/lora_evo_ta_all_layers_16'\n",
        "\n",
        "model_reloaded = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                                      trust_remote_code=True).to('cuda:0')\n",
        "model_reloaded.config.use_cache = True\n",
        "model_reloaded.eval()"
      ],
      "metadata": {
        "id": "xkdkP44gvZq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evo_tokenizer=AutoTokenizer.from_pretrained(\"togethercomputer/evo-1-8k-base\",trust_remote_code=True)\n",
        "#tokenizer_reload = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True)\n",
        "evo_tokenizer.add_special_tokens({'eos_token': ' '})\n",
        "evo_tokenizer.pad_token = evo_tokenizer.eos_token"
      ],
      "metadata": {
        "id": "ARXWBXEFFhjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='@!'\n",
        "run_model(model_reloaded, evo_tokenizer, prompt)"
      ],
      "metadata": {
        "id": "2dZGINv20z25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EvmISeJnhQ5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B-jX8o4zhQ75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BNsfoJqyhQ9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LCbM3h0ohRAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N0IuG2_shRB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evo = HF_Evo()"
      ],
      "metadata": {
        "id": "0UB4JKf2hZmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='@!'\n",
        "run_model(evo.model, evo.tokenizer, prompt)"
      ],
      "metadata": {
        "id": "cQl8HvPyJzs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vGZqVmM0JzlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'lsmille/lora_evo_ta_all_layers_13'\n",
        "\n",
        "#config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
        "model_reloaded2 = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                                       config=config,\n",
        "                                                       revision=revision,\n",
        "                                                       trust_remote_code=True).to('cuda:0')\n",
        "model_reloaded2.config.use_cache = True\n",
        "model_reloaded2.eval()"
      ],
      "metadata": {
        "id": "7AuUuTrDJzha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt='@!'\n",
        "run_model(model_reloaded2, evo_tokenizer, prompt)"
      ],
      "metadata": {
        "id": "tEU2C6aHJzfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "97ZIMpJDJzdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fZmvKY9AJzbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GC5c7EHxJzZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from trl import SFTTrainer\n",
        "\n",
        "# max_seq_length = 1024\n",
        "\n",
        "# trainer = SFTTrainer(\n",
        "#     model=evo.model,\n",
        "#     train_dataset=dataset,\n",
        "#     eval_dataset=dataset_test,\n",
        "#     peft_config=lora_config,\n",
        "#     dataset_text_field=\"text\",\n",
        "#     max_seq_length=max_seq_length,\n",
        "#     tokenizer=evo.tokenizer,\n",
        "#     args=training_args,\n",
        "# )"
      ],
      "metadata": {
        "id": "EqzGoV9RAsxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trainer.train()"
      ],
      "metadata": {
        "id": "fOGJysr9CJVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is if you dont load lora in model same as commenst two above\n",
        "#also change get_peft_model\n",
        "\n",
        "# from trl import SFTTrainer\n",
        "\n",
        "# max_seq_length = 512\n",
        "\n",
        "# trainer = SFTTrainer(\n",
        "#     model=evo.model,\n",
        "#     train_dataset=dataset,\n",
        "#     peft_config=lora_config, #lora config is here\n",
        "#     dataset_text_field=\"text\",\n",
        "#     max_seq_length=max_seq_length,\n",
        "#     tokenizer=evo.tokenizer,\n",
        "#     args=training_args,\n",
        "# )\n",
        "\n",
        "# trainer.train()\n",
        "\n",
        "#try same inference but with new model"
      ],
      "metadata": {
        "id": "--6jsfkT92rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O5IutdT5f7wZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}